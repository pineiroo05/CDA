-Cada vez q se cambia algo de algun servicio, hay q reiniciarlo-

-Parte iscsi-
tgtadm --lld iscsi --op show --mode target -> para ver si lo hago bien
1) Configuracion target iscsi (discos)
a)target asignando iqn y tid
    En tid es un numero q yo quiera
    tgtadm --lld iscsi --op new --mode target --tid 1 --target iqn.2025-10.net.cda.discos:prueba
b) Lun dentro del target al /dev/sdf
    tgtadm --lld iscsi --op new --mode logicalunit --tid 1 --lun 1 --backing-store /dev/sdf
c) Control de acceso
    -Restringir acceso al target
    tgtadm --lld iscsi --op bind --mode target --tid 1 --initiator-address 192.168.100.22
    tgtadm --lld iscsi --op bind --mode target --tid 1 --initiator-address 192.168.100.33
    -Autenticacion
    tgtadm --lld iscsi --op new --mode account --user cda --password cdapass
2) Configuracion y uso de initiators (cliente1 y cliente2)
a) Renombrar initiators
    echo "InitiatorName="`iscsi-iname` > /etc/iscsi/initiatorname.iscsi
b) Habilitar autenticacion con chap
    nano /etc/iscsi/iscsid.conf, y descomentar las lineas node.session.auth.authmethod = CHAP / node.session.auth.username = cda / node.session.auth.password = cdapass
c) Recargar el servicio
    systemctl restart iscsid.service
d) Descubrir los target
    iscsiadm -m discovery -t sendtargets -p 192.168.100.11
    iscsiadm -m node -> para ver lo que descubrio
e) Conectar al target
    iscsiadm -m node -T iqn.2025-10.net.cda.discos:prueba -p 192.168.100.11 --login

-Parte iscsi+ocfs2 sobre raid-
a) Crear raid 10 con varios discos
mdadm --create /dev/md0 --level=10 --raid-devices=4 /dev/sdc /dev/sdd /dev/sde /dev/sdf
b) Crear un grupo de volúmenes LVM empleando como volumen físico el array RAID-10 anterior y definir tres volúmenes lógicos con 50 MB de capacidad cada uno (con los nombres UNO, DOS, COMPARTIDO)
-> pvcreate /dev/md0
-> vgcreate VG_DATOS /dev/md0
-> lvcreate -n UNO -L 50 VG_DATOS
   lvcreate -n DOS -L 50 VG_DATOS
   lvcreate -n COMPARTIDO -L 50 VG_DATOS
-> crear los target y todo el tema de acceso x ip
   tgtadm --lld iscsi --op new --mode target --tid 2 --targetname iqn.2025-10.local.discos:uno
   tgtadm --lld iscsi --op new --mode logicalunit --tid 2 --lun 1 --backing-store /dev/VG_DATOS/UNO
   tgtadm --lld iscsi --op bind --mode target --tid 2 --initiator-address 192.168.100.22

   tgtadm --lld iscsi --op new --mode target --tid 3 --targetname iqn.2025-10.local.discos:dos
   tgtadm --lld iscsi --op new --mode logicalunit --tid 3 --lun 1 --backing-store /dev/VG_DATOS/DOS
   tgtadm --lld iscsi --op bind --mode target --tid 3 --initiator-address 192.168.100.33

   tgtadm --lld iscsi --op new --mode target --tid 4 --targetname iqn.2025-10.local.discos:compartido
   tgtadm --lld iscsi --op new --mode logicalunit --tid 4 --lun 1 --backing-store /dev/VG_DATOS/COMPARTIDO
   tgtadm --lld iscsi --op bind --mode target --tid 4 --initiator-address ALL

c) Configurar y conectar initiators de cliente1 y cliente2 (en sus respectivas mvs)
iscsiadm -m discovery -t sendtargets -p 192.168.100.11
iscsiadm -m node -T (IQNs EN CADA CASO) --login
d) Formatear y montar (tmb se hace en las vm de los usuarios)
o2cb add-cluster clustercda
o2cb add-node clustercda cliente1 --ip 192.168.100.22
o2cb add-node clustercda cliente2 --ip 192.168.100.33 
Para ver q disco esta en cada uno -> ls -la /dev/disk/by-path | grep iscsi
-> cliente 1, uno
mkfs -t ext3 /dev/sdc
mkdir -p /mnt/uno
mount /dev/sdc /mnt/uno
-> cliente1, compartido ###IMPORTANTE VERIFICAR SI OCFS2 ESTA ACTIVE (service ocfs2 status)
mkfs -t ocfs2 /dev/sdd
mkdir -p /mnt/compartido
mount -t ocfs2 /dev/sdd /mnt/compartido
-> cliente2, dos (aunq en los 2 el disco tenga el mismo nombre, son diferentes)
mkfs -t ext3 /dev/sdc
mkdir -p /mnt/uno
mount /dev/sdc /mnt/uno
-> cliente2, compartido
mkdir -p /mnt/compartido
mount -t ocfs2 /dev/sdd /mnt/compartido
d) Persistencia
nano /etc/default/o2cb (ambos clientes)
O2CB_ENABLED=true
O2CB_BOOTCLUSTER=clustercda
